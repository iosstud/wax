"use strict";(globalThis.webpackChunkwax_docs=globalThis.webpackChunkwax_docs||[]).push([[451],{2942(e,i,n){n.r(i),n.d(i,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>o});const t=JSON.parse('{"id":"mini-lm/mini-lm-embedder","title":"MiniLM Embedder","description":"Set up, configure, and optimize the on-device MiniLM embedding provider.","source":"@site/docs/mini-lm/mini-lm-embedder.md","sourceDirName":"mini-lm","slug":"/mini-lm/mini-lm-embedder","permalink":"/docs/mini-lm/mini-lm-embedder","draft":false,"unlisted":false,"editUrl":"https://github.com/christopherkarani/Wax/tree/main/website/docs/mini-lm/mini-lm-embedder.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"MiniLM Embedder","sidebar_label":"MiniLM Embedder"},"sidebar":"docs","previous":{"title":"Embedding Providers","permalink":"/docs/vector-search/embedding-providers"}}');var d=n(4848),r=n(8453);const s={sidebar_position:1,title:"MiniLM Embedder",sidebar_label:"MiniLM Embedder"},l=void 0,c={},o=[{value:"Overview",id:"overview",level:2},{value:"Setup",id:"setup",level:2},{value:"Prewarming",id:"prewarming",level:2},{value:"Single vs. Batch Embedding",id:"single-vs-batch-embedding",level:2},{value:"Batch Planning",id:"batch-planning",level:3},{value:"Tokenization",id:"tokenization",level:2},{value:"Sequence Length Optimization",id:"sequence-length-optimization",level:3},{value:"CoreML Integration",id:"coreml-integration",level:2},{value:"Compute Units",id:"compute-units",level:3},{value:"Diagnostics",id:"diagnostics",level:3},{value:"Model Output",id:"model-output",level:3},{value:"Performance Notes",id:"performance-notes",level:2},{value:"Memory Profile",id:"memory-profile",level:3},{value:"Thread Safety",id:"thread-safety",level:3},{value:"Embedding Identity",id:"embedding-identity",level:2}];function a(e){const i={code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,d.jsxs)(d.Fragment,{children:[(0,d.jsx)(i.p,{children:"Set up, configure, and optimize the on-device MiniLM embedding provider."}),"\n",(0,d.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,d.jsxs)(i.p,{children:[(0,d.jsx)(i.code,{children:"MiniLMEmbedder"})," is an actor that implements both ",(0,d.jsx)(i.code,{children:"EmbeddingProvider"})," and ",(0,d.jsx)(i.code,{children:"BatchEmbeddingProvider"}),". It manages CoreML model loading, tokenization, and inference with automatic batch optimization."]}),"\n",(0,d.jsx)(i.h2,{id:"setup",children:"Setup"}),"\n",(0,d.jsx)(i.p,{children:"Create an embedder with default settings:"}),"\n",(0,d.jsx)(i.pre,{children:(0,d.jsx)(i.code,{className:"language-swift",children:"let embedder = try MiniLMEmbedder()\n"})}),"\n",(0,d.jsx)(i.p,{children:"Or customize the batch size and CoreML configuration:"}),"\n",(0,d.jsx)(i.pre,{children:(0,d.jsx)(i.code,{className:"language-swift",children:"var config = MiniLMEmbedder.Config()\nconfig.batchSize = 128  // Default is 256\n\nlet mlConfig = MLModelConfiguration()\nmlConfig.computeUnits = .cpuAndNeuralEngine\n\nconfig.modelConfiguration = mlConfig\n\nlet embedder = try MiniLMEmbedder(config: config)\n"})}),"\n",(0,d.jsx)(i.h2,{id:"prewarming",children:"Prewarming"}),"\n",(0,d.jsx)(i.p,{children:"The first inference triggers JIT compilation of the CoreML model, which adds latency. Prewarm the model during app launch to avoid this:"}),"\n",(0,d.jsx)(i.pre,{children:(0,d.jsx)(i.code,{className:"language-swift",children:"try await embedder.prewarm(batchSize: 16)\n"})}),"\n",(0,d.jsx)(i.p,{children:"The batch size is clamped to 1...32 for prewarming."}),"\n",(0,d.jsx)(i.h2,{id:"single-vs-batch-embedding",children:"Single vs. Batch Embedding"}),"\n",(0,d.jsx)(i.p,{children:"For individual queries, use the single-text API:"}),"\n",(0,d.jsx)(i.pre,{children:(0,d.jsx)(i.code,{className:"language-swift",children:'let vector = try await embedder.embed("search query")\n'})}),"\n",(0,d.jsx)(i.p,{children:"For bulk ingestion, batch embedding is significantly faster:"}),"\n",(0,d.jsx)(i.pre,{children:(0,d.jsx)(i.code,{className:"language-swift",children:'let texts = ["doc 1", "doc 2", /* ... thousands more ... */]\nlet vectors = try await embedder.embed(batch: texts)\n'})}),"\n",(0,d.jsx)(i.h3,{id:"batch-planning",children:"Batch Planning"}),"\n",(0,d.jsxs)(i.p,{children:["The embedder splits large batches into chunks based on the configured ",(0,d.jsx)(i.code,{children:"batchSize"})," (default 256). Each chunk is processed as a single CoreML prediction with buffer reuse to minimize allocations."]}),"\n",(0,d.jsxs)(i.p,{children:["For a batch of 1,000 texts with ",(0,d.jsx)(i.code,{children:"batchSize = 256"}),":"]}),"\n",(0,d.jsxs)(i.ul,{children:["\n",(0,d.jsx)(i.li,{children:"3 full batches of 256"}),"\n",(0,d.jsx)(i.li,{children:"1 remainder batch of 232"}),"\n",(0,d.jsx)(i.li,{children:"Size-1 batches fall back to single-text inference"}),"\n"]}),"\n",(0,d.jsx)(i.h2,{id:"tokenization",children:"Tokenization"}),"\n",(0,d.jsx)(i.p,{children:"Text is tokenized using the full BERT WordPiece pipeline:"}),"\n",(0,d.jsxs)(i.ol,{children:["\n",(0,d.jsxs)(i.li,{children:[(0,d.jsx)(i.strong,{children:"Basic tokenization"})," \u2014 Whitespace/punctuation splitting, diacritic normalization, lowercasing"]}),"\n",(0,d.jsxs)(i.li,{children:[(0,d.jsx)(i.strong,{children:"WordPiece tokenization"})," \u2014 Greedy longest-match subword splitting with ",(0,d.jsx)(i.code,{children:"##"})," continuation prefix"]}),"\n",(0,d.jsxs)(i.li,{children:[(0,d.jsx)(i.strong,{children:"Special token wrapping"})," \u2014 ",(0,d.jsx)(i.code,{children:"[CLS]"})," prepended, ",(0,d.jsx)(i.code,{children:"[SEP]"})," appended"]}),"\n",(0,d.jsxs)(i.li,{children:[(0,d.jsx)(i.strong,{children:"Padding"})," \u2014 Zero-padded to the selected sequence length"]}),"\n"]}),"\n",(0,d.jsx)(i.h3,{id:"sequence-length-optimization",children:"Sequence Length Optimization"}),"\n",(0,d.jsx)(i.p,{children:"Instead of always padding to 512 tokens, the tokenizer selects the smallest bucket that fits the longest text in each batch:"}),"\n",(0,d.jsxs)(i.table,{children:[(0,d.jsx)(i.thead,{children:(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.th,{children:"Bucket"}),(0,d.jsx)(i.th,{children:"Use Case"})]})}),(0,d.jsxs)(i.tbody,{children:[(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.td,{children:"32"}),(0,d.jsx)(i.td,{children:"Short phrases"})]}),(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.td,{children:"64"}),(0,d.jsx)(i.td,{children:"Single sentences"})]}),(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.td,{children:"128"}),(0,d.jsx)(i.td,{children:"Short paragraphs"})]}),(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.td,{children:"256"}),(0,d.jsx)(i.td,{children:"Long paragraphs"})]}),(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.td,{children:"384"}),(0,d.jsx)(i.td,{children:"Multi-paragraph text"})]}),(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.td,{children:"512"}),(0,d.jsx)(i.td,{children:"Maximum (full BERT limit)"})]})]})]}),"\n",(0,d.jsx)(i.p,{children:"This reduces computation by 2-4x for typical inputs."}),"\n",(0,d.jsx)(i.h2,{id:"coreml-integration",children:"CoreML Integration"}),"\n",(0,d.jsx)(i.h3,{id:"compute-units",children:"Compute Units"}),"\n",(0,d.jsxs)(i.p,{children:["The default compute unit configuration is ",(0,d.jsx)(i.code,{children:".cpuAndNeuralEngine"}),", which routes transformer attention operations to Apple's Neural Engine for optimal throughput. This is 1.5-2x faster than ",(0,d.jsx)(i.code,{children:".all"})," (which includes the GPU) because it avoids GPU dispatch overhead."]}),"\n",(0,d.jsx)(i.h3,{id:"diagnostics",children:"Diagnostics"}),"\n",(0,d.jsx)(i.p,{children:"Check which compute hardware is being used:"}),"\n",(0,d.jsx)(i.pre,{children:(0,d.jsx)(i.code,{className:"language-swift",children:"let usesANE = try await embedder.isUsingANE()\nlet units = try await embedder.currentComputeUnits()\n"})}),"\n",(0,d.jsx)(i.h3,{id:"model-output",children:"Model Output"}),"\n",(0,d.jsxs)(i.p,{children:["The CoreML model outputs Float16 embeddings, which are converted to Float32 using Accelerate's ",(0,d.jsx)(i.code,{children:"vDSP.convertElements"})," (8-16x faster than scalar conversion). The vectors are L2-normalized before returning."]}),"\n",(0,d.jsx)(i.h2,{id:"performance-notes",children:"Performance Notes"}),"\n",(0,d.jsxs)(i.table,{children:[(0,d.jsx)(i.thead,{children:(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.th,{children:"Operation"}),(0,d.jsx)(i.th,{children:"Typical Latency"})]})}),(0,d.jsxs)(i.tbody,{children:[(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.td,{children:"Single embed"}),(0,d.jsx)(i.td,{children:"~50-100ms (ANE)"})]}),(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.td,{children:"Batch (256 texts)"}),(0,d.jsx)(i.td,{children:"~2-4s (ANE)"})]}),(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.td,{children:"Prewarming"}),(0,d.jsx)(i.td,{children:"~500ms-1s"})]}),(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.td,{children:"Model loading"}),(0,d.jsx)(i.td,{children:"~200-500ms (cached after first load)"})]})]})]}),"\n",(0,d.jsx)(i.h3,{id:"memory-profile",children:"Memory Profile"}),"\n",(0,d.jsxs)(i.table,{children:[(0,d.jsx)(i.thead,{children:(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.th,{children:"Component"}),(0,d.jsx)(i.th,{children:"Size"})]})}),(0,d.jsxs)(i.tbody,{children:[(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.td,{children:"CoreML model"}),(0,d.jsx)(i.td,{children:"~50 MiB"})]}),(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.td,{children:"Tokenizer vocab"}),(0,d.jsx)(i.td,{children:"~200 KiB"})]}),(0,d.jsxs)(i.tr,{children:[(0,d.jsx)(i.td,{children:"Batch buffers"}),(0,d.jsx)(i.td,{children:"~1 MiB per 256-item batch"})]})]})]}),"\n",(0,d.jsx)(i.h3,{id:"thread-safety",children:"Thread Safety"}),"\n",(0,d.jsxs)(i.p,{children:[(0,d.jsx)(i.code,{children:"MiniLMEmbedder"})," is an actor, so all methods are safe to call concurrently from any context. The underlying CoreML model uses a thread-safe cache to prevent concurrent model loads (which can cause CoreML/Espresso deadlocks)."]}),"\n",(0,d.jsx)(i.h2,{id:"embedding-identity",children:"Embedding Identity"}),"\n",(0,d.jsx)(i.p,{children:"The embedder reports its identity for provenance tracking:"}),"\n",(0,d.jsx)(i.pre,{children:(0,d.jsx)(i.code,{className:"language-swift",children:'embedder.identity\n// EmbeddingIdentity(\n//     provider: "Wax",\n//     model: "MiniLMAll",\n//     dimensions: 384,\n//     normalized: true\n// )\n'})}),"\n",(0,d.jsx)(i.p,{children:"This identity is stored alongside vector indexes to detect embedding model changes between sessions."})]})}function h(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,d.jsx)(i,{...e,children:(0,d.jsx)(a,{...e})}):a(e)}},8453(e,i,n){n.d(i,{R:()=>s,x:()=>l});var t=n(6540);const d={},r=t.createContext(d);function s(e){const i=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(d):e.components||d:s(e.components),t.createElement(r.Provider,{value:i},e.children)}}}]);